#!/usr/bin/env python3
"""
è™•ç† ePub é›»å­æ›¸ä¸¦åŠ å…¥çŸ¥è­˜åº«
"""
import os
import re
import json
import zipfile
from pathlib import Path
from html.parser import HTMLParser

# ePub æª”æ¡ˆé…ç½®
EPUB_DIR = Path.home() / "Documents/é›»å­æ›¸/Eddieé›»å­æ›¸/å‘½ç†æ›¸ç±/å‘½ç†ç›¸é—œepub word"
OUTPUT_DIR = Path.home() / "Projects/jgeizhun/knowledge-base"

EPUB_BOOKS = {
    "å…«å­—å‘½ç†å­¸é€²éšæ•™ç¨‹_nodrm.epub": {
        "name": "å…«å­—å‘½ç†å­¸é€²éšæ•™ç¨‹",
        "category": "å…«å­—",
        "author": "é™¸è‡´æ¥µ",
        "dynasty": "ç¾ä»£"
    },
    "å­å¹³çœŸè¯ ï¼ˆåŸæœ¬ï¼‰_nodrm.epub": {
        "name": "å­å¹³çœŸè©®ï¼ˆåŸæœ¬ï¼‰",
        "category": "å…«å­—",
        "author": "æ²ˆå­ç»",
        "dynasty": "æ¸…"
    },
    "ä¸‰å‘½é€šä¼š.epub": {
        "name": "ä¸‰å‘½é€šæœƒ",
        "category": "å…«å­—",
        "author": "è¬æ°‘è‹±",
        "dynasty": "æ˜"
    },
    "ç´«å¾®å››åŒ–_nodrm.epub": {
        "name": "ç´«å¾®å››åŒ–",
        "category": "ç´«å¾®",
        "author": "ç‹æ–‡è¯",
        "dynasty": "ç¾ä»£"
    },
    "ç´«å¾®æ¢æº_nodrm.epub": {
        "name": "ç´«å¾®æ¢æº",
        "category": "ç´«å¾®",
        "author": "ç‹æ–‡è¯",
        "dynasty": "ç¾ä»£"
    },
    "å‚…ä½©æ¦®çš„æ˜“ç¶“å…¥é–€èª²ï¼ˆä¸‰ç‰ˆï¼‰(å®Œæ•´)_nodrm.epub": {
        "name": "å‚…ä½©æ¦®æ˜“ç¶“å…¥é–€èª²",
        "category": "æ˜“ç¶“",
        "author": "å‚…ä½©æ¦®",
        "dynasty": "ç¾ä»£"
    }
}

class HTMLTextExtractor(HTMLParser):
    """å¾ HTML ä¸­æå–ç´”æ–‡å­—"""
    def __init__(self):
        super().__init__()
        self.text = []
        self.skip_tags = {'script', 'style', 'head', 'meta', 'link'}
        self.current_skip = False
        self.in_paragraph = False
    
    def handle_starttag(self, tag, attrs):
        if tag in self.skip_tags:
            self.current_skip = True
        if tag in ('p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li', 'br'):
            self.text.append('\n')
    
    def handle_endtag(self, tag):
        if tag in self.skip_tags:
            self.current_skip = False
        if tag in ('p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'):
            self.text.append('\n')
    
    def handle_data(self, data):
        if not self.current_skip:
            self.text.append(data)
    
    def get_text(self):
        return ''.join(self.text)

def extract_text_from_html(html_content):
    """å¾ HTML å…§å®¹æå–æ–‡å­—"""
    parser = HTMLTextExtractor()
    try:
        parser.feed(html_content)
        return parser.get_text()
    except:
        # å‚™ç”¨æ–¹æ¡ˆï¼šç°¡å–®çš„æ­£å‰‡è¡¨é”å¼
        text = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<[^>]+>', ' ', text)
        text = re.sub(r'&nbsp;', ' ', text)
        text = re.sub(r'&[a-z]+;', '', text)
        return text

def parse_epub(epub_path):
    """è§£æ ePub æª”æ¡ˆï¼Œæå–æ‰€æœ‰ç« ç¯€æ–‡å­—"""
    chapters = []
    
    with zipfile.ZipFile(epub_path, 'r') as zf:
        # æ‰¾å‡ºæ‰€æœ‰ HTML/XHTML æª”æ¡ˆ
        html_files = []
        for name in zf.namelist():
            if name.endswith(('.html', '.xhtml', '.htm')) and 'toc' not in name.lower():
                html_files.append(name)
        
        # å˜—è©¦å¾ OPF æª”æ¡ˆä¸­ç²å–æ­£ç¢ºé †åº
        opf_file = None
        for name in zf.namelist():
            if name.endswith('.opf'):
                opf_file = name
                break
        
        ordered_files = []
        if opf_file:
            opf_content = zf.read(opf_file).decode('utf-8', errors='ignore')
            # æå– spine é †åº
            spine_match = re.search(r'<spine[^>]*>(.*?)</spine>', opf_content, re.DOTALL)
            if spine_match:
                itemrefs = re.findall(r'idref="([^"]+)"', spine_match.group(1))
                # å¾ manifest ç²å– id åˆ° href çš„æ˜ å°„
                manifest_match = re.search(r'<manifest>(.*?)</manifest>', opf_content, re.DOTALL)
                if manifest_match:
                    id_to_href = {}
                    items = re.findall(r'<item\s+[^>]*id="([^"]+)"[^>]*href="([^"]+)"', manifest_match.group(1))
                    for item_id, href in items:
                        id_to_href[item_id] = href
                    
                    # æŒ‰ spine é †åºæ’åˆ—æª”æ¡ˆ
                    opf_dir = os.path.dirname(opf_file)
                    for idref in itemrefs:
                        if idref in id_to_href:
                            href = id_to_href[idref]
                            if opf_dir:
                                full_path = opf_dir + '/' + href
                            else:
                                full_path = href
                            # æ¨™æº–åŒ–è·¯å¾‘
                            full_path = full_path.replace('//', '/')
                            if full_path in html_files or any(h.endswith(href) for h in html_files):
                                for h in html_files:
                                    if h.endswith(href) or h == full_path:
                                        if h not in ordered_files:
                                            ordered_files.append(h)
                                        break
        
        # å¦‚æœç„¡æ³•å¾ OPF ç²å–é †åºï¼Œä½¿ç”¨æ–‡ä»¶åæ’åº
        if not ordered_files:
            ordered_files = sorted(html_files)
        
        # è®€å–æ¯å€‹ HTML æª”æ¡ˆ
        for i, html_file in enumerate(ordered_files):
            try:
                content = zf.read(html_file).decode('utf-8', errors='ignore')
                text = extract_text_from_html(content)
                text = clean_text(text)
                
                if len(text.strip()) < 50:
                    continue
                
                # å˜—è©¦å¾å…§å®¹ä¸­æå–ç« ç¯€æ¨™é¡Œ
                title = extract_chapter_title(text, html_file, i+1)
                
                chapters.append({
                    "chapter": f"ç¬¬{i+1}ç« ",
                    "title": title,
                    "content": text.strip()
                })
            except Exception as e:
                print(f"    âš ï¸ ç„¡æ³•è®€å– {html_file}: {e}")
    
    return chapters

def clean_text(text):
    """æ¸…ç†æ–‡å­—"""
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    text = re.sub(r'[ \t]+', ' ', text)
    # ç§»é™¤å¤šé¤˜æ›è¡Œ
    text = re.sub(r'\n{3,}', '\n\n', text)
    # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
    lines = [line.strip() for line in text.split('\n')]
    text = '\n'.join(lines)
    return text.strip()

def extract_chapter_title(text, filename, chapter_num):
    """å¾æ–‡å­—ä¸­æå–ç« ç¯€æ¨™é¡Œ"""
    lines = text.strip().split('\n')
    
    # æ‰¾ç¬¬ä¸€è¡Œéç©ºæ–‡å­—ä½œç‚ºæ¨™é¡Œ
    for line in lines[:5]:
        line = line.strip()
        if line and len(line) < 100:
            # æ¸…ç†æ¨™é¡Œ
            title = re.sub(r'^\d+[\.ã€\s]+', '', line)  # ç§»é™¤é–‹é ­çš„æ•¸å­—
            title = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« ç¯€ç¯‡å›]\s*', '', title)  # ç§»é™¤ç« ç¯€æ¨™è¨˜
            if title:
                return title[:50]
    
    # å¾æ–‡ä»¶åæå–
    name = os.path.basename(filename)
    name = re.sub(r'\.(html|xhtml|htm)$', '', name, flags=re.IGNORECASE)
    name = re.sub(r'^\d+[-_]?', '', name)
    if name and name not in ['index', 'cover', 'copyright', 'toc']:
        return name[:50]
    
    return f"ç¬¬{chapter_num}ç¯€"

def merge_short_chapters(chapters, min_length=500):
    """åˆä½µéçŸ­çš„ç« ç¯€"""
    merged = []
    buffer = None
    
    for chapter in chapters:
        content = chapter["content"]
        
        if buffer:
            # èˆ‡ä¸Šä¸€å€‹ç« ç¯€åˆä½µ
            buffer["content"] += "\n\n" + content
            buffer["title"] += " / " + chapter["title"]
            
            if len(buffer["content"]) >= min_length:
                merged.append(buffer)
                buffer = None
        elif len(content) < min_length:
            buffer = chapter.copy()
        else:
            merged.append(chapter)
    
    if buffer:
        merged.append(buffer)
    
    return merged

def extract_keywords(text):
    """å¾æ–‡æœ¬ä¸­æå–é—œéµè©"""
    keywords = set()
    
    # å…«å­—ç›¸é—œ
    bazi_terms = [
        "å¤©å¹²", "åœ°æ”¯", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸",
        "å­", "ä¸‘", "å¯…", "å¯", "è¾°", "å·³", "åˆ", "æœª", "ç”³", "é…‰", "æˆŒ", "äº¥",
        "é™°é™½", "äº”è¡Œ", "é‡‘", "æœ¨", "æ°´", "ç«", "åœŸ",
        "ç›¸ç”Ÿ", "ç›¸å‰‹", "ç”Ÿå…‹",
        "å°ç¶¬", "æ¯”è‚©", "åŠ«è²¡", "é£Ÿç¥", "å‚·å®˜", "åè²¡", "æ­£è²¡", "åå®˜", "æ­£å®˜", "ä¸ƒæ®º",
        "æ ¼å±€", "ç”¨ç¥", "å–œç¥", "å¿Œç¥",
        "é•·ç”Ÿ", "æ²æµ´", "å† å¸¶", "è‡¨å®˜", "å¸æ—º", "è¡°", "ç—…", "æ­»", "å¢“", "çµ•", "èƒ", "é¤Š",
        "æ—¥ä¸»", "æœˆä»¤", "èª¿å€™", "å¤§é‹", "æµå¹´"
    ]
    
    # ç´«å¾®ç›¸é—œ
    ziwei_terms = [
        "ç´«å¾®", "å¤©åºœ", "å¤ªé™½", "å¤ªé™°", "æ­¦æ›²", "å¤©åŒ", "å»‰è²", "å¤©æ©Ÿ",
        "è²ªç‹¼", "å·¨é–€", "å¤©ç›¸", "å¤©æ¢", "ä¸ƒæ®º", "ç ´è»",
        "æ–‡æ˜Œ", "æ–‡æ›²", "å·¦è¼”", "å³å¼¼", "å¤©é­", "å¤©é‰",
        "ç¥¿å­˜", "å¤©é¦¬", "æ“ç¾Š", "é™€ç¾…", "ç«æ˜Ÿ", "éˆ´æ˜Ÿ",
        "åŒ–ç¥¿", "åŒ–æ¬Š", "åŒ–ç§‘", "åŒ–å¿Œ", "å››åŒ–",
        "å‘½å®®", "å…„å¼Ÿå®®", "å¤«å¦»å®®", "å­å¥³å®®", "è²¡å¸›å®®", "ç–¾å„å®®",
        "é·ç§»å®®", "äº¤å‹å®®", "äº‹æ¥­å®®", "ç”°å®…å®®", "ç¦å¾·å®®", "çˆ¶æ¯å®®",
        "å®®æ°£", "é£›åŒ–", "ç–Šå®®"
    ]
    
    # æ˜“ç¶“ç›¸é—œ
    yijing_terms = [
        "å¤ªæ¥µ", "å…©å„€", "å››è±¡", "å…«å¦",
        "ä¹¾", "å¤", "éœ‡", "å·½", "å", "é›¢", "è‰®", "å…Œ",
        "å…­åå››å¦", "çˆ»", "å¦è¾­", "çˆ»è¾­",
        "é«”ç”¨", "å‹•çˆ»", "è®Šå¦", "å åœ"
    ]
    
    all_terms = bazi_terms + ziwei_terms + yijing_terms
    for term in all_terms:
        if term in text:
            keywords.add(term)
    
    return list(keywords)[:20]

def process_epub_book(epub_filename, config):
    """è™•ç†å–®æœ¬ ePub æ›¸ç±"""
    epub_path = EPUB_DIR / epub_filename
    
    if not epub_path.exists():
        print(f"  âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {epub_path}")
        return []
    
    print(f"  ğŸ“– è™•ç†: {config['name']}")
    
    # è§£æ ePub
    chapters = parse_epub(epub_path)
    
    if not chapters:
        print(f"    âš ï¸ ç„¡æ³•æå–ä»»ä½•ç« ç¯€")
        return []
    
    # åˆä½µéçŸ­çš„ç« ç¯€
    chapters = merge_short_chapters(chapters)
    
    print(f"    ğŸ“„ æå– {len(chapters)} å€‹ç« ç¯€")
    
    # ç”ŸæˆçŸ¥è­˜åº«æ¢ç›®
    entries = []
    for i, chapter in enumerate(chapters):
        entry_id = f"{config['name'].replace(' ', '_')}_{i+1:03d}"
        
        entry = {
            "id": entry_id,
            "source": config["name"],
            "category": config["category"],
            "chapter": chapter["chapter"],
            "title": chapter["title"],
            "content": chapter["content"],
            "keywords": extract_keywords(chapter["content"]),
            "metadata": {
                "author": config["author"],
                "dynasty": config["dynasty"],
                "quality": 5,
                "format": "epub"
            }
        }
        entries.append(entry)
    
    return entries

def generate_rag_chunks(entries):
    """ç”Ÿæˆé©åˆ RAG ä½¿ç”¨çš„åˆ†å¡Š"""
    chunks = []
    chunk_size = 1000  # æ¯å¡Šç´„ 1000 å­—
    
    for entry in entries:
        content = entry["content"]
        
        # å¦‚æœå…§å®¹ä¸é•·ï¼Œç›´æ¥ä½œç‚ºä¸€å¡Š
        if len(content) <= chunk_size:
            chunks.append({
                "id": f"{entry['id']}_chunk_001",
                "text": content,
                "source": entry["source"],
                "chapter": entry["chapter"],
                "title": entry["title"],
                "category": entry["category"],
                "keywords": entry["keywords"]
            })
        else:
            # åˆ†æˆå¤šå¡Š
            paragraphs = content.split('\n\n')
            current_chunk = []
            current_length = 0
            chunk_num = 1
            
            for para in paragraphs:
                if current_length + len(para) > chunk_size and current_chunk:
                    chunks.append({
                        "id": f"{entry['id']}_chunk_{chunk_num:03d}",
                        "text": '\n\n'.join(current_chunk),
                        "source": entry["source"],
                        "chapter": entry["chapter"],
                        "title": entry["title"],
                        "category": entry["category"],
                        "keywords": entry["keywords"]
                    })
                    chunk_num += 1
                    current_chunk = []
                    current_length = 0
                
                current_chunk.append(para)
                current_length += len(para)
            
            # ä¿å­˜æœ€å¾Œä¸€å¡Š
            if current_chunk:
                chunks.append({
                    "id": f"{entry['id']}_chunk_{chunk_num:03d}",
                    "text": '\n\n'.join(current_chunk),
                    "source": entry["source"],
                    "chapter": entry["chapter"],
                    "title": entry["title"],
                    "category": entry["category"],
                    "keywords": entry["keywords"]
                })
    
    return chunks

def save_markdown(entry, output_dir):
    """å„²å­˜ç‚º Markdown æ ¼å¼"""
    book_dir = output_dir / entry["category"] / entry["source"].replace(" ", "_")
    book_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç†æ¨™é¡Œ
    safe_title = entry['title'][:30].strip()
    safe_chapter = entry['chapter'].replace('/', '_')
    filename = f"{safe_chapter}_{safe_title}.md"
    # ç§»é™¤æª”åä¸­çš„éæ³•å­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*\n\r]', '_', filename)
    
    filepath = book_dir / filename
    
    keywords_str = json.dumps(entry["keywords"], ensure_ascii=False)
    
    content = f"""---
source: {entry["source"]}
chapter: {entry["chapter"]}
title: {entry["title"]}
category: {entry["category"]}
author: {entry["metadata"]["author"]}
dynasty: {entry["metadata"]["dynasty"]}
format: epub
keywords: {keywords_str}
---

# {entry["title"]}

{entry["content"]}
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

def main():
    print("ğŸš€ é–‹å§‹è™•ç† ePub é›»å­æ›¸...\n")
    
    all_entries = []
    
    for epub_filename, config in EPUB_BOOKS.items():
        entries = process_epub_book(epub_filename, config)
        all_entries.extend(entries)
        
        # å„²å­˜ Markdown
        for entry in entries:
            save_markdown(entry, OUTPUT_DIR)
    
    if not all_entries:
        print("\nâŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æ›¸ç±")
        return
    
    # è®€å–ç¾æœ‰çš„ rag_chunks.json
    chunks_path = OUTPUT_DIR / "rag_chunks.json"
    existing_chunks = []
    existing_sources = set()
    
    if chunks_path.exists():
        with open(chunks_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            existing_chunks = data.get("chunks", [])
            
            # è¨˜éŒ„ç¾æœ‰ä¾†æºï¼Œé¿å…é‡è¤‡
            for chunk in existing_chunks:
                existing_sources.add(chunk.get("source", ""))
    
    # éæ¿¾æ‰å·²å­˜åœ¨çš„æ›¸ç±
    new_entries = []
    for entry in all_entries:
        if entry["source"] not in existing_sources:
            new_entries.append(entry)
    
    if not new_entries:
        print("\nâš ï¸ æ‰€æœ‰æ›¸ç±å·²ç¶“åœ¨çŸ¥è­˜åº«ä¸­")
        return
    
    # ç”Ÿæˆæ–°çš„ RAG åˆ†å¡Š
    new_chunks = generate_rag_chunks(new_entries)
    
    # åˆä½µåˆ†å¡Š
    all_chunks = existing_chunks + new_chunks
    
    # å„²å­˜æ›´æ–°å¾Œçš„ rag_chunks.json
    with open(chunks_path, 'w', encoding='utf-8') as f:
        json.dump({
            "version": "1.0",
            "total_chunks": len(all_chunks),
            "chunks": all_chunks
        }, f, ensure_ascii=False, indent=2)
    
    # æ›´æ–° index.json
    index_path = OUTPUT_DIR / "index.json"
    if index_path.exists():
        with open(index_path, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        # æ·»åŠ æ–°æ¢ç›®
        existing_entries = index_data.get("entries", [])
        existing_entries.extend(new_entries)
        
        # æ›´æ–°æ›¸ç±åˆ—è¡¨
        existing_books = set((b["name"], b["category"]) for b in index_data.get("books", []))
        for entry in new_entries:
            book_tuple = (entry["source"], entry["category"])
            if book_tuple not in existing_books:
                index_data.setdefault("books", []).append({
                    "name": entry["source"],
                    "category": entry["category"]
                })
        
        index_data["entries"] = existing_entries
        index_data["total_entries"] = len(existing_entries)
        index_data["total_chunks"] = len(all_chunks)
        
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"ğŸ“Š æ–°å¢çµ±è¨ˆï¼š")
    print(f"   - æ–°å¢ç« ç¯€æ¢ç›®: {len(new_entries)}")
    print(f"   - æ–°å¢ RAG åˆ†å¡Š: {len(new_chunks)}")
    print(f"   - ç¸½ RAG åˆ†å¡Š: {len(all_chunks)}")

if __name__ == "__main__":
    main()
