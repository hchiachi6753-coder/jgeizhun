#!/usr/bin/env python3
"""
ç®—å‘½å¤æ›¸çŸ¥è­˜åº«è™•ç†è…³æœ¬
å°‡ OCR å¥½çš„å¤æ›¸æ–‡å­—æ•´ç†æˆçµæ§‹åŒ–çš„ JSON çŸ¥è­˜åº«
"""
import os
import re
import json
from pathlib import Path

SOURCE_DIR = Path.home() / "Documents/ç®—å‘½æ›¸/æ–‡å­—æª”"
OUTPUT_DIR = Path.home() / "Projects/jgeizhun/knowledge-base"

# æ›¸ç±é…ç½®
BOOKS = {
    "å…«å­—": {
        "å­å¹³çœŸè©®": {
            "file": "å…«å­—/å­å¹³çœŸè©®.txt",
            "author": "æ²ˆå­ç»",
            "dynasty": "æ¸…",
            "quality": 5,
            "chapter_pattern": r"^ç¬¬(\d+)ç« \s+(.+)$"
        },
        "çª®é€šå¯¶é‘‘": {
            "file": "å…«å­—/çª®é€šå¯¶é‘‘.txt",
            "author": "ä½™æ˜¥å°",
            "dynasty": "æ¸…",
            "quality": 5,
            "chapter_pattern": r"^ç¬¬(\w+)éƒ¨[ï¼š:]\s*(.+)$|^(\d+\.\d+)\s+(.+)$"
        },
        "æ·µæµ·å­å¹³": {
            "file": "å…«å­—/æ·µæµ·å­å¹³.txt",
            "author": "å¾å­å¹³",
            "dynasty": "å®‹",
            "quality": 5,
            "chapter_pattern": r"^(\w+)[ã€.]\s*(.+)$"
        },
        "ä¸‰å‘½é€šæœƒ": {
            "file": "å…«å­—/ä¸‰å‘½é€šæœƒ.txt",
            "author": "è¬æ°‘è‹±",
            "dynasty": "æ˜",
            "quality": 5,
            "chapter_pattern": r"^ç¬¬(\d+)ç« \s+(.+)$"
        },
        "åƒé‡Œå‘½ç¨¿": {
            "file": "å…«å­—/åƒé‡Œå‘½ç¨¿.txt",
            "author": "éŸ‹åƒé‡Œ",
            "dynasty": "æ°‘åœ‹",
            "quality": 5,
            "chapter_pattern": r"^(\w+)[ã€.]\s*(.+)$"
        },
        "å…«å­—å‘½ç†å­¸é€²éšæ•™ç¨‹": {
            "file": "å…«å­—/å…«å­—å‘½ç†å­¸é€²éšæ•™ç¨‹.txt",
            "author": "é™¸è‡´æ¥µ",
            "dynasty": "ç¾ä»£",
            "quality": 5,
            "chapter_pattern": r"^(åº\w+|ç¬¬\w+ç« )\s*(.*)$"
        },
        "æ»´å¤©é«“è£œæ³¨": {
            "file": "å…«å­—/æ»´å¤©é«“è£œæ³¨.txt",
            "author": "ä»»éµæ¨µ",
            "dynasty": "æ¸…",
            "quality": 3,
            "chapter_pattern": None
        }
    },
    "ç´«å¾®": {
        "ç´«å¾®å››åŒ–": {
            "file": "ç´«å¾®/ç´«å¾®å››åŒ–.txt",
            "author": "ä¸è©³",
            "dynasty": "ç¾ä»£",
            "quality": 5,
            "chapter_pattern": r"^ç¬¬(\w+)ç« \s+(.+)$|^ç¬¬(\w+)ç¯€\s+(.+)$"
        },
        "ç´«å¾®æ¢æº": {
            "file": "ç´«å¾®/ç´«å¾®æ¢æº.txt",
            "author": "ä¸è©³",
            "dynasty": "ç¾ä»£",
            "quality": 5,
            "chapter_pattern": r"^ç¬¬(\w+)ç¯‡\s+(.+)$|^ç¬¬(\w+)ç« \s+(.+)$"
        }
    },
    "æ˜“ç¶“": {
        "å‚…ä½©æ¦®æ˜“ç¶“å…¥é–€èª²": {
            "file": "å…«å­—/å‚…ä½©æ¦®æ˜“ç¶“å…¥é–€èª².txt",
            "author": "å‚…ä½©æ¦®",
            "dynasty": "ç¾ä»£",
            "quality": 5,
            "chapter_pattern": r"^(å·\w+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.])\s*(.+)$"
        },
        "æ¢…èŠ±æ˜“æ•¸": {
            "file": "å…«å­—/æ¢…èŠ±æ˜“æ•¸.txt",
            "author": "é‚µåº·ç¯€",
            "dynasty": "å®‹ï¼ˆç¾ä»£è§£æï¼‰",
            "quality": 5,
            "chapter_pattern": r"^ç¬¬(\w+)éƒ¨åˆ†[ï¼š:]\s*(.+)$|^(\d+\.\d+)\s+(.+)$"
        },
        "æ˜“ç¶“é›œèªª": {
            "file": "å…«å­—/æ˜“ç¶“é›œèªª.txt",
            "author": "å—æ‡·ç‘¾",
            "dynasty": "ç¾ä»£",
            "quality": 5,
            "chapter_pattern": None
        },
        "åœç­®æ­£å®—": {
            "file": "å…«å­—/åœç­®æ­£å®—.txt",
            "author": "ç‹æ´ªç·’",
            "dynasty": "æ¸…",
            "quality": 4,
            "chapter_pattern": None
        }
    }
}

def clean_ocr_text(text):
    """æ¸…ç† OCR å¸¸è¦‹éŒ¯èª¤"""
    # ç§»é™¤å¸¸è¦‹çš„ OCR äº‚ç¢¼ç¬¦è™Ÿ
    text = re.sub(r'[ï¹ï¹’ï¹”ï¹•ï¹–ï¹—ï¹›ï¹œï¹ï¹ï¹Ÿï¹ ï¹¡ï¹¢ï¹£ï¹¤ï¹¥ï¹¦ï¹¨ï¹©ï¹ªï¹«]', '', text)
    # ç§»é™¤ä¸å¯æ‰“å°å­—ç¬¦
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    # æ¨™æº–åŒ–ç©ºç™½
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    # ä¿®æ­£å¸¸è¦‹ OCR éŒ¯èª¤
    text = text.replace('è¼¿', 'èˆ‡')
    text = text.replace('æ–¼', 'äº')  # æœ‰äº›åœ°æ–¹éœ€è¦ä¿ç•™
    return text.strip()

def split_into_sections(text, pattern):
    """å°‡æ–‡æœ¬æŒ‰ç« ç¯€åˆ†æ®µ"""
    if not pattern:
        # æ²’æœ‰ç« ç¯€æ¨¡å¼ï¼Œç”¨æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        sections = []
        current_section = {"title": "å…¨æ–‡", "content": []}
        for p in paragraphs:
            if len(p) > 50:  # æœ‰æ„ç¾©çš„æ®µè½
                current_section["content"].append(p)
        if current_section["content"]:
            sections.append(current_section)
        return sections
    
    sections = []
    current_section = None
    current_content = []
    
    for line in text.split('\n'):
        match = re.match(pattern, line.strip(), re.MULTILINE)
        if match:
            # ä¿å­˜å‰ä¸€å€‹ç« ç¯€
            if current_section:
                current_section["content"] = '\n'.join(current_content).strip()
                if current_section["content"]:
                    sections.append(current_section)
            
            # é–‹å§‹æ–°ç« ç¯€
            groups = [g for g in match.groups() if g]
            chapter_num = groups[0] if groups else ""
            chapter_title = groups[1] if len(groups) > 1 else groups[0]
            
            current_section = {
                "chapter": chapter_num,
                "title": chapter_title.strip()
            }
            current_content = []
        else:
            current_content.append(line)
    
    # ä¿å­˜æœ€å¾Œä¸€å€‹ç« ç¯€
    if current_section:
        current_section["content"] = '\n'.join(current_content).strip()
        if current_section["content"]:
            sections.append(current_section)
    
    return sections

def extract_keywords(text):
    """å¾æ–‡æœ¬ä¸­æå–é—œéµè©"""
    keywords = set()
    
    # å…«å­—ç›¸é—œ
    bazi_terms = [
        "å¤©å¹²", "åœ°æ”¯", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸",
        "å­", "ä¸‘", "å¯…", "å¯", "è¾°", "å·³", "åˆ", "æœª", "ç”³", "é…‰", "æˆŒ", "äº¥",
        "é™°é™½", "äº”è¡Œ", "é‡‘", "æœ¨", "æ°´", "ç«", "åœŸ",
        "ç›¸ç”Ÿ", "ç›¸å‰‹", "ç”Ÿå…‹", "ç”Ÿå‰‹",
        "å°ç¶¬", "æ¯”è‚©", "åŠ«è²¡", "é£Ÿç¥", "å‚·å®˜", "åè²¡", "æ­£è²¡", "åå®˜", "æ­£å®˜", "ä¸ƒæ®º",
        "æ ¼å±€", "ç”¨ç¥", "å–œç¥", "å¿Œç¥",
        "é•·ç”Ÿ", "æ²æµ´", "å† å¸¶", "è‡¨å®˜", "å¸æ—º", "è¡°", "ç—…", "æ­»", "å¢“", "çµ•", "èƒ", "é¤Š",
        "æ—¥ä¸»", "æœˆä»¤", "èª¿å€™", "å¤§é‹", "æµå¹´"
    ]
    
    # ç´«å¾®ç›¸é—œ
    ziwei_terms = [
        "ç´«å¾®", "å¤©åºœ", "å¤ªé™½", "å¤ªé™°", "æ­¦æ›²", "å¤©åŒ", "å»‰è²", "å¤©æ©Ÿ",
        "è²ªç‹¼", "å·¨é–€", "å¤©ç›¸", "å¤©æ¢", "ä¸ƒæ®º", "ç ´è»",
        "æ–‡æ˜Œ", "æ–‡æ›²", "å·¦è¼”", "å³å¼¼", "å¤©é­", "å¤©é‰",
        "ç¥¿å­˜", "å¤©é¦¬", "æ“ç¾Š", "é™€ç¾…", "ç«æ˜Ÿ", "éˆ´æ˜Ÿ",
        "åŒ–ç¥¿", "åŒ–æ¬Š", "åŒ–ç§‘", "åŒ–å¿Œ", "å››åŒ–",
        "å‘½å®®", "å…„å¼Ÿå®®", "å¤«å¦»å®®", "å­å¥³å®®", "è²¡å¸›å®®", "ç–¾å„å®®",
        "é·ç§»å®®", "äº¤å‹å®®", "äº‹æ¥­å®®", "ç”°å®…å®®", "ç¦å¾·å®®", "çˆ¶æ¯å®®"
    ]
    
    # æ˜“ç¶“ç›¸é—œ
    yijing_terms = [
        "å¤ªæ¥µ", "å…©å„€", "å››è±¡", "å…«å¦",
        "ä¹¾", "å¤", "éœ‡", "å·½", "å", "é›¢", "è‰®", "å…Œ",
        "å…­åå››å¦", "çˆ»", "å¦è¾­", "çˆ»è¾­",
        "é«”ç”¨", "å‹•çˆ»", "è®Šå¦"
    ]
    
    all_terms = bazi_terms + ziwei_terms + yijing_terms
    for term in all_terms:
        if term in text:
            keywords.add(term)
    
    return list(keywords)[:20]  # æœ€å¤šè¿”å›20å€‹é—œéµè©

def process_book(category, book_name, config):
    """è™•ç†å–®æœ¬æ›¸ç±"""
    file_path = SOURCE_DIR / config["file"]
    if not file_path.exists():
        print(f"  âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        return []
    
    print(f"  ğŸ“– è™•ç†: {book_name}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # æ¸…ç† OCR æ–‡æœ¬
    text = clean_ocr_text(text)
    
    # åˆ†æ®µ
    sections = split_into_sections(text, config.get("chapter_pattern"))
    
    # ç”ŸæˆçŸ¥è­˜åº«æ¢ç›®
    entries = []
    for i, section in enumerate(sections):
        entry_id = f"{book_name.replace(' ', '_')}_{i+1:03d}"
        
        content = section.get("content", "")
        if isinstance(content, list):
            content = '\n\n'.join(content)
        
        # è·³éå¤ªçŸ­çš„å…§å®¹
        if len(content) < 100:
            continue
        
        entry = {
            "id": entry_id,
            "source": book_name,
            "category": category,
            "chapter": section.get("chapter", str(i+1)),
            "title": section.get("title", f"ç¬¬{i+1}ç¯€"),
            "content": content,
            "keywords": extract_keywords(content),
            "metadata": {
                "author": config.get("author", "ä¸è©³"),
                "dynasty": config.get("dynasty", "ä¸è©³"),
                "quality": config.get("quality", 3)
            }
        }
        entries.append(entry)
    
    print(f"    âœ… æå– {len(entries)} å€‹ç« ç¯€")
    return entries

def save_markdown(entry, output_dir):
    """å„²å­˜ç‚º Markdown æ ¼å¼"""
    book_dir = output_dir / entry["category"] / entry["source"].replace(" ", "_")
    book_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"{entry['chapter']}_{entry['title'][:20]}.md"
    # ç§»é™¤æª”åä¸­çš„éæ³•å­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    
    filepath = book_dir / filename
    
    content = f"""---
source: {entry["source"]}
chapter: {entry["chapter"]}
title: {entry["title"]}
category: {entry["category"]}
author: {entry["metadata"]["author"]}
dynasty: {entry["metadata"]["dynasty"]}
keywords: {entry["keywords"]}
---

# {entry["title"]}

{entry["content"]}
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

def main():
    print("ğŸš€ é–‹å§‹è™•ç†ç®—å‘½å¤æ›¸çŸ¥è­˜åº«...\n")
    
    all_entries = []
    
    for category, books in BOOKS.items():
        print(f"\nğŸ“š è™•ç†é¡åˆ¥: {category}")
        for book_name, config in books.items():
            entries = process_book(category, book_name, config)
            all_entries.extend(entries)
            
            # å„²å­˜ Markdown
            for entry in entries:
                save_markdown(entry, OUTPUT_DIR)
    
    # å„²å­˜ JSON ç´¢å¼•
    index_path = OUTPUT_DIR / "index.json"
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump({
            "version": "1.0",
            "total_entries": len(all_entries),
            "categories": list(BOOKS.keys()),
            "entries": all_entries
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæˆï¼å…±è™•ç† {len(all_entries)} å€‹æ¢ç›®")
    print(f"ğŸ“ è¼¸å‡ºä½ç½®: {OUTPUT_DIR}")
    print(f"ğŸ“„ ç´¢å¼•æª”æ¡ˆ: {index_path}")

if __name__ == "__main__":
    main()
