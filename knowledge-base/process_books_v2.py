#!/usr/bin/env python3
"""
ç®—å‘½å¤æ›¸çŸ¥è­˜åº«è™•ç†è…³æœ¬ v2
æ”¹é€²ç« ç¯€åˆ†å‰²é‚è¼¯ï¼Œæ”¯æ´æ›´å¤šæ ¼å¼
"""
import os
import re
import json
from pathlib import Path

SOURCE_DIR = Path.home() / "Documents/ç®—å‘½æ›¸/æ–‡å­—æª”"
OUTPUT_DIR = Path.home() / "Projects/jgeizhun/knowledge-base"

# æ›¸ç±é…ç½®
BOOKS = {
    "å…«å­—": {
        "å­å¹³çœŸè©®": {
            "file": "å…«å­—/å­å¹³çœŸè©®.txt",
            "author": "æ²ˆå­ç»",
            "dynasty": "æ¸…",
            "quality": 5
        },
        "çª®é€šå¯¶é‘‘": {
            "file": "å…«å­—/çª®é€šå¯¶é‘‘.txt",
            "author": "ä½™æ˜¥å°",
            "dynasty": "æ¸…",
            "quality": 5
        },
        "æ·µæµ·å­å¹³": {
            "file": "å…«å­—/æ·µæµ·å­å¹³.txt",
            "author": "å¾å­å¹³",
            "dynasty": "å®‹",
            "quality": 5
        },
        "ä¸‰å‘½é€šæœƒ": {
            "file": "å…«å­—/ä¸‰å‘½é€šæœƒ.txt",
            "author": "è¬æ°‘è‹±",
            "dynasty": "æ˜",
            "quality": 5
        },
        "åƒé‡Œå‘½ç¨¿": {
            "file": "å…«å­—/åƒé‡Œå‘½ç¨¿.txt",
            "author": "éŸ‹åƒé‡Œ",
            "dynasty": "æ°‘åœ‹",
            "quality": 5
        },
        "å…«å­—å‘½ç†å­¸é€²éšæ•™ç¨‹": {
            "file": "å…«å­—/å…«å­—å‘½ç†å­¸é€²éšæ•™ç¨‹.txt",
            "author": "é™¸è‡´æ¥µ",
            "dynasty": "ç¾ä»£",
            "quality": 5
        }
    },
    "ç´«å¾®": {
        "ç´«å¾®å››åŒ–": {
            "file": "ç´«å¾®/ç´«å¾®å››åŒ–.txt",
            "author": "ç‹æ–‡è¯",
            "dynasty": "ç¾ä»£",
            "quality": 5
        },
        "ç´«å¾®æ¢æº": {
            "file": "ç´«å¾®/ç´«å¾®æ¢æº.txt",
            "author": "ç‹æ–‡è¯",
            "dynasty": "ç¾ä»£",
            "quality": 5
        }
    },
    "æ˜“ç¶“": {
        "å‚…ä½©æ¦®æ˜“ç¶“å…¥é–€èª²": {
            "file": "å…«å­—/å‚…ä½©æ¦®æ˜“ç¶“å…¥é–€èª².txt",
            "author": "å‚…ä½©æ¦®",
            "dynasty": "ç¾ä»£",
            "quality": 5
        },
        "æ¢…èŠ±æ˜“æ•¸": {
            "file": "å…«å­—/æ¢…èŠ±æ˜“æ•¸.txt",
            "author": "é‚µåº·ç¯€",
            "dynasty": "å®‹ï¼ˆç¾ä»£è§£æï¼‰",
            "quality": 5
        },
        "æ˜“ç¶“é›œèªª": {
            "file": "å…«å­—/æ˜“ç¶“é›œèªª.txt",
            "author": "å—æ‡·ç‘¾",
            "dynasty": "ç¾ä»£",
            "quality": 5
        }
    }
}

def clean_ocr_text(text):
    """æ¸…ç† OCR å¸¸è¦‹éŒ¯èª¤"""
    # ç§»é™¤å¸¸è¦‹çš„ OCR äº‚ç¢¼ç¬¦è™Ÿ
    text = re.sub(r'[ï¹ï¹’ï¹”ï¹•ï¹–ï¹—ï¹›ï¹œï¹ï¹ï¹Ÿï¹ ï¹¡ï¹¢ï¹£ï¹¤ï¹¥ï¹¦ï¹¨ï¹©ï¹ªï¹«]', '', text)
    # ç§»é™¤ä¸å¯æ‰“å°å­—ç¬¦
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    # æ¨™æº–åŒ–ç©ºç™½
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()

def smart_split(text, book_name):
    """æ™ºèƒ½ç« ç¯€åˆ†å‰²"""
    sections = []
    
    # å„ç¨®ç« ç¯€æ¨¡å¼
    patterns = [
        # ç¬¬Xç«  æ¨™é¡Œ
        (r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« )\s*[:ï¼š]?\s*(.+?)$', 'chapter'),
        # ç¬¬Xç¯‡ æ¨™é¡Œ
        (r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯‡)\s*[:ï¼š]?\s*(.+?)$', 'part'),
        # ç¬¬Xç¯€ æ¨™é¡Œ
        (r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€)\s*[:ï¼š]?\s*(.+?)$', 'section'),
        # X.X æ¨™é¡Œ
        (r'^(\d+\.\d+)\s+(.+?)$', 'subsection'),
        # ä¸€ã€æ¨™é¡Œ / 1ã€æ¨™é¡Œ
        (r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+)[ã€.]\s*(.+?)$', 'item'),
    ]
    
    lines = text.split('\n')
    current_section = {"chapter": "å‰è¨€", "title": "å‰è¨€", "content": []}
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            current_section["content"].append("")
            continue
        
        matched = False
        for pattern, ptype in patterns:
            match = re.match(pattern, line)
            if match:
                # æª¢æŸ¥é€™æ˜¯å¦åªæ˜¯ç›®éŒ„æ¢ç›®ï¼ˆä¸‹ä¸€è¡Œä¹Ÿæ˜¯ç« ç¯€æ¨™é¡Œï¼‰
                next_line = lines[i+1].strip() if i+1 < len(lines) else ""
                is_toc = any(re.match(p[0], next_line) for p in patterns)
                
                # å¦‚æœç•¶å‰å…§å®¹å¾ˆå°‘ä¸”ä¸‹ä¸€è¡Œä¹Ÿæ˜¯ç« ç¯€æ¨™é¡Œï¼Œå¯èƒ½æ˜¯ç›®éŒ„
                if is_toc and len('\n'.join(current_section["content"]).strip()) < 50:
                    continue
                
                # ä¿å­˜å‰ä¸€å€‹ç« ç¯€
                content_text = '\n'.join(current_section["content"]).strip()
                if len(content_text) >= 100:
                    current_section["content"] = content_text
                    sections.append(current_section)
                
                # é–‹å§‹æ–°ç« ç¯€
                current_section = {
                    "chapter": match.group(1),
                    "title": match.group(2).strip() if len(match.groups()) > 1 else match.group(1),
                    "content": []
                }
                matched = True
                break
        
        if not matched:
            current_section["content"].append(line)
    
    # ä¿å­˜æœ€å¾Œä¸€å€‹ç« ç¯€
    content_text = '\n'.join(current_section["content"]).strip()
    if len(content_text) >= 100:
        current_section["content"] = content_text
        sections.append(current_section)
    
    # å¦‚æœæ²’æœ‰åˆ†å‡ºç« ç¯€ï¼ŒæŒ‰å›ºå®šé•·åº¦åˆ†æ®µ
    if len(sections) < 3:
        sections = split_by_paragraphs(text, 2000)  # æ¯æ®µç´„ 2000 å­—
    
    return sections

def split_by_paragraphs(text, max_chars=2000):
    """æŒ‰æ®µè½å’Œå­—æ•¸åˆ†å‰²"""
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    sections = []
    current_section = {"chapter": "1", "title": "ç¬¬1éƒ¨åˆ†", "content": []}
    current_length = 0
    section_num = 1
    
    for para in paragraphs:
        if current_length + len(para) > max_chars and current_section["content"]:
            # ä¿å­˜ç•¶å‰æ®µè½
            current_section["content"] = '\n\n'.join(current_section["content"])
            sections.append(current_section)
            
            # é–‹å§‹æ–°æ®µè½
            section_num += 1
            current_section = {"chapter": str(section_num), "title": f"ç¬¬{section_num}éƒ¨åˆ†", "content": []}
            current_length = 0
        
        current_section["content"].append(para)
        current_length += len(para)
    
    # ä¿å­˜æœ€å¾Œä¸€å€‹æ®µè½
    if current_section["content"]:
        current_section["content"] = '\n\n'.join(current_section["content"])
        sections.append(current_section)
    
    return sections

def extract_keywords(text):
    """å¾æ–‡æœ¬ä¸­æå–é—œéµè©"""
    keywords = set()
    
    # å…«å­—ç›¸é—œ
    bazi_terms = [
        "å¤©å¹²", "åœ°æ”¯", "ç”²", "ä¹™", "ä¸™", "ä¸", "æˆŠ", "å·±", "åºš", "è¾›", "å£¬", "ç™¸",
        "å­", "ä¸‘", "å¯…", "å¯", "è¾°", "å·³", "åˆ", "æœª", "ç”³", "é…‰", "æˆŒ", "äº¥",
        "é™°é™½", "äº”è¡Œ", "é‡‘", "æœ¨", "æ°´", "ç«", "åœŸ",
        "ç›¸ç”Ÿ", "ç›¸å‰‹", "ç”Ÿå…‹",
        "å°ç¶¬", "æ¯”è‚©", "åŠ«è²¡", "é£Ÿç¥", "å‚·å®˜", "åè²¡", "æ­£è²¡", "åå®˜", "æ­£å®˜", "ä¸ƒæ®º",
        "æ ¼å±€", "ç”¨ç¥", "å–œç¥", "å¿Œç¥",
        "é•·ç”Ÿ", "æ²æµ´", "å† å¸¶", "è‡¨å®˜", "å¸æ—º", "è¡°", "ç—…", "æ­»", "å¢“", "çµ•", "èƒ", "é¤Š",
        "æ—¥ä¸»", "æœˆä»¤", "èª¿å€™", "å¤§é‹", "æµå¹´"
    ]
    
    # ç´«å¾®ç›¸é—œ
    ziwei_terms = [
        "ç´«å¾®", "å¤©åºœ", "å¤ªé™½", "å¤ªé™°", "æ­¦æ›²", "å¤©åŒ", "å»‰è²", "å¤©æ©Ÿ",
        "è²ªç‹¼", "å·¨é–€", "å¤©ç›¸", "å¤©æ¢", "ä¸ƒæ®º", "ç ´è»",
        "æ–‡æ˜Œ", "æ–‡æ›²", "å·¦è¼”", "å³å¼¼", "å¤©é­", "å¤©é‰",
        "ç¥¿å­˜", "å¤©é¦¬", "æ“ç¾Š", "é™€ç¾…", "ç«æ˜Ÿ", "éˆ´æ˜Ÿ",
        "åŒ–ç¥¿", "åŒ–æ¬Š", "åŒ–ç§‘", "åŒ–å¿Œ", "å››åŒ–",
        "å‘½å®®", "å…„å¼Ÿå®®", "å¤«å¦»å®®", "å­å¥³å®®", "è²¡å¸›å®®", "ç–¾å„å®®",
        "é·ç§»å®®", "äº¤å‹å®®", "äº‹æ¥­å®®", "ç”°å®…å®®", "ç¦å¾·å®®", "çˆ¶æ¯å®®",
        "å®®æ°£", "é£›åŒ–", "ç–Šå®®"
    ]
    
    # æ˜“ç¶“ç›¸é—œ
    yijing_terms = [
        "å¤ªæ¥µ", "å…©å„€", "å››è±¡", "å…«å¦",
        "ä¹¾", "å¤", "éœ‡", "å·½", "å", "é›¢", "è‰®", "å…Œ",
        "å…­åå››å¦", "çˆ»", "å¦è¾­", "çˆ»è¾­",
        "é«”ç”¨", "å‹•çˆ»", "è®Šå¦", "å åœ"
    ]
    
    all_terms = bazi_terms + ziwei_terms + yijing_terms
    for term in all_terms:
        if term in text:
            keywords.add(term)
    
    return list(keywords)[:20]

def process_book(category, book_name, config):
    """è™•ç†å–®æœ¬æ›¸ç±"""
    file_path = SOURCE_DIR / config["file"]
    if not file_path.exists():
        print(f"  âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
        return []
    
    print(f"  ğŸ“– è™•ç†: {book_name}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # æ¸…ç† OCR æ–‡æœ¬
    text = clean_ocr_text(text)
    
    # æ™ºèƒ½åˆ†æ®µ
    sections = smart_split(text, book_name)
    
    # ç”ŸæˆçŸ¥è­˜åº«æ¢ç›®
    entries = []
    for i, section in enumerate(sections):
        entry_id = f"{book_name.replace(' ', '_')}_{i+1:03d}"
        
        content = section.get("content", "")
        if isinstance(content, list):
            content = '\n\n'.join(content)
        
        # è·³éå¤ªçŸ­çš„å…§å®¹
        if len(content) < 100:
            continue
        
        entry = {
            "id": entry_id,
            "source": book_name,
            "category": category,
            "chapter": section.get("chapter", str(i+1)),
            "title": section.get("title", f"ç¬¬{i+1}ç¯€"),
            "content": content,
            "keywords": extract_keywords(content),
            "metadata": {
                "author": config.get("author", "ä¸è©³"),
                "dynasty": config.get("dynasty", "ä¸è©³"),
                "quality": config.get("quality", 3)
            }
        }
        entries.append(entry)
    
    print(f"    âœ… æå– {len(entries)} å€‹ç« ç¯€")
    return entries

def save_markdown(entry, output_dir):
    """å„²å­˜ç‚º Markdown æ ¼å¼"""
    book_dir = output_dir / entry["category"] / entry["source"].replace(" ", "_")
    book_dir.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç†æ¨™é¡Œ
    safe_title = entry['title'][:30].strip()
    safe_chapter = entry['chapter'].replace('/', '_')
    filename = f"{safe_chapter}_{safe_title}.md"
    # ç§»é™¤æª”åä¸­çš„éæ³•å­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*\n\r]', '_', filename)
    
    filepath = book_dir / filename
    
    keywords_str = json.dumps(entry["keywords"], ensure_ascii=False)
    
    content = f"""---
source: {entry["source"]}
chapter: {entry["chapter"]}
title: {entry["title"]}
category: {entry["category"]}
author: {entry["metadata"]["author"]}
dynasty: {entry["metadata"]["dynasty"]}
keywords: {keywords_str}
---

# {entry["title"]}

{entry["content"]}
"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)

def generate_rag_chunks(entries):
    """ç”Ÿæˆé©åˆ RAG ä½¿ç”¨çš„åˆ†å¡Š"""
    chunks = []
    chunk_size = 1000  # æ¯å¡Šç´„ 1000 å­—
    
    for entry in entries:
        content = entry["content"]
        
        # å¦‚æœå…§å®¹ä¸é•·ï¼Œç›´æ¥ä½œç‚ºä¸€å¡Š
        if len(content) <= chunk_size:
            chunks.append({
                "id": f"{entry['id']}_chunk_001",
                "text": content,
                "source": entry["source"],
                "chapter": entry["chapter"],
                "title": entry["title"],
                "category": entry["category"],
                "keywords": entry["keywords"]
            })
        else:
            # åˆ†æˆå¤šå¡Š
            paragraphs = content.split('\n\n')
            current_chunk = []
            current_length = 0
            chunk_num = 1
            
            for para in paragraphs:
                if current_length + len(para) > chunk_size and current_chunk:
                    chunks.append({
                        "id": f"{entry['id']}_chunk_{chunk_num:03d}",
                        "text": '\n\n'.join(current_chunk),
                        "source": entry["source"],
                        "chapter": entry["chapter"],
                        "title": entry["title"],
                        "category": entry["category"],
                        "keywords": entry["keywords"]
                    })
                    chunk_num += 1
                    current_chunk = []
                    current_length = 0
                
                current_chunk.append(para)
                current_length += len(para)
            
            # ä¿å­˜æœ€å¾Œä¸€å¡Š
            if current_chunk:
                chunks.append({
                    "id": f"{entry['id']}_chunk_{chunk_num:03d}",
                    "text": '\n\n'.join(current_chunk),
                    "source": entry["source"],
                    "chapter": entry["chapter"],
                    "title": entry["title"],
                    "category": entry["category"],
                    "keywords": entry["keywords"]
                })
    
    return chunks

def main():
    print("ğŸš€ é–‹å§‹è™•ç†ç®—å‘½å¤æ›¸çŸ¥è­˜åº« v2...\n")
    
    all_entries = []
    
    for category, books in BOOKS.items():
        print(f"\nğŸ“š è™•ç†é¡åˆ¥: {category}")
        for book_name, config in books.items():
            entries = process_book(category, book_name, config)
            all_entries.extend(entries)
            
            # å„²å­˜ Markdown
            for entry in entries:
                save_markdown(entry, OUTPUT_DIR)
    
    # ç”Ÿæˆ RAG åˆ†å¡Š
    rag_chunks = generate_rag_chunks(all_entries)
    
    # å„²å­˜ JSON ç´¢å¼•
    index_path = OUTPUT_DIR / "index.json"
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump({
            "version": "2.0",
            "total_entries": len(all_entries),
            "total_chunks": len(rag_chunks),
            "categories": list(BOOKS.keys()),
            "books": [{"name": book, "category": cat} for cat, books in BOOKS.items() for book in books],
            "entries": all_entries
        }, f, ensure_ascii=False, indent=2)
    
    # å„²å­˜ RAG åˆ†å¡Š
    chunks_path = OUTPUT_DIR / "rag_chunks.json"
    with open(chunks_path, 'w', encoding='utf-8') as f:
        json.dump({
            "version": "1.0",
            "total_chunks": len(rag_chunks),
            "chunks": rag_chunks
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œæˆï¼")
    print(f"ğŸ“Š çµ±è¨ˆï¼š")
    print(f"   - ç« ç¯€æ¢ç›®: {len(all_entries)}")
    print(f"   - RAG åˆ†å¡Š: {len(rag_chunks)}")
    print(f"ğŸ“ è¼¸å‡ºä½ç½®: {OUTPUT_DIR}")
    print(f"ğŸ“„ ç´¢å¼•æª”æ¡ˆ: {index_path}")
    print(f"ğŸ“„ RAG åˆ†å¡Š: {chunks_path}")

if __name__ == "__main__":
    main()
